{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment Checker
",
    "
",
    "This notebook allows you to interactively check the audio-to-phoneme alignment for segmented recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os
",
    "import sys
",
    "from pathlib import Path
",
    "import numpy as np
",
    "import pandas as pd
",
    "import librosa
",
    "import librosa.display
",
    "import matplotlib.pyplot as plt
",
    "import ipywidgets as widgets
",
    "from IPython.display import Audio, display
",
    "
",
    "# Add project root to sys.path
",
    "sys.path.append(os.path.abspath('..'))
",
    "
",
    "from data_prepare.audio_utils import get_sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION
",
    "DATA_DIR = '../datalocal/v260210_24kHz/readtext_split'
",
    "
",
    "if not Path(DATA_DIR).exists():
",
    "    print(f"Warning: DATA_DIR {DATA_DIR} not found. Please run the segmentation script first.")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speaker_data(data_dir):
",
    "    data_dir = Path(data_dir)
",
    "    all_files = list(data_dir.glob("*.wav"))
",
    "    
",
    "    # Group by speaker (e.g., 001PD)
",
    "    speakers = {}
",
    "    for f in all_files:
",
    "        speaker_id = f.stem.split('_')[0]
",
    "        if speaker_id not in speakers:
",
    "            speakers[speaker_id] = []
",
    "        
",
    "        # Get text for duration/stats
",
    "        txt_path = f.with_suffix('.txt')
",
    "        text = ""
",
    "        if txt_path.exists():
",
    "            with open(txt_path, 'r', encoding='utf-8') as tf:
",
    "                text = tf.read()
",
    "        
",
    "        speakers[speaker_id].append({
",
    "            'stem': f.stem,
",
    "            'text': text
",
    "        })
",
    "    
",
    "    return speakers
",
    "
",
    "SPEAKER_DATA = get_speaker_data(DATA_DIR)
",
    "SPEAKER_LIST = sorted(list(SPEAKER_DATA.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_alignment(stem):
",
    "    wav_path = Path(DATA_DIR) / f"{stem}.wav"
",
    "    csv_path = Path(DATA_DIR) / f"{stem}.csv"
",
    "    
",
    "    if not wav_path.exists() or not csv_path.exists():
",
    "        print(f"Error: Missing files for {stem}")
",
    "        return
",
    "    
",
    "    # Load audio
",
    "    y, sr = librosa.load(wav_path, sr=None)
",
    "    duration = len(y) / sr
",
    "    
",
    "    # Load alignment
",
    "    df = pd.read_csv(csv_path, sep=';')
",
    "    
",
    "    # Create figure
",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 8), sharex=True)
",
    "    
",
    "    # 1. Waveform
",
    "    librosa.display.waveshow(y, sr=sr, ax=ax1, alpha=0.5)
",
    "    ax1.set_title(f"Alignment for {stem}")
",
    "    ax1.set_ylabel("Amplitude")
",
    "    
",
    "    # 2. Spectrogram
",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)
",
    "    img = librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='hz', ax=ax2)
",
    "    ax2.set_ylabel("Hz")
",
    "    
",
    "    # Draw boundaries and labels
",
    "    # Words (unique tokens)
",
    "    word_boundaries = df.groupby('TOKEN').agg({'BEGIN': 'min', 'DURATION': 'sum', 'ORT': 'first'}).reset_index()
",
    "    word_boundaries = word_boundaries[word_boundaries['TOKEN'] >= 0]
",
    "    
",
    "    # Vertical lines for phonemes (MAU)
",
    "    for _, row in df.iterrows():
",
    "        start_sec = row['BEGIN'] / sr
",
    "        end_sec = (row['BEGIN'] + row['DURATION']) / sr
",
    "        
",
    "        # Phoneme boundary (dashed)
",
    "        ax1.axvline(start_sec, color='gray', linestyle='--', alpha=0.3)
",
    "        ax2.axvline(start_sec, color='white', linestyle='--', alpha=0.3)
",
    "        
",
    "        # Label phoneme
",
    "        mid_sec = start_sec + (row['DURATION'] / (2 * sr))
",
    "        ax1.text(mid_sec, 0.8, row['MAU'], color='red', fontsize=8, horizontalalignment='center')
",
    "
",
    "    # Vertical lines for words (TOKEN)
",
    "    for _, row in word_boundaries.iterrows():
",
    "        start_sec = row['BEGIN'] / sr
",
    "        end_sec = (row['BEGIN'] + row['DURATION']) / sr
",
    "        
",
    "        # Word boundary (solid)
",
    "        ax1.axvline(start_sec, color='blue', linewidth=2, alpha=0.6)
",
    "        ax2.axvline(start_sec, color='cyan', linewidth=2, alpha=0.6)
",
    "        
",
    "        # Label word
",
    "        mid_sec = start_sec + (row['DURATION'] / (2 * sr))
",
    "        ax1.text(mid_sec, -0.9, row['ORT'], color='blue', fontsize=12, fontweight='bold', horizontalalignment='center')
",
    "
",
    "    plt.tight_layout()
",
    "    plt.show()
",
    "    
",
    "    # Audio player
",
    "    display(Audio(y, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTERACTIVE WIDGETS
",
    "speaker_select = widgets.Dropdown(options=SPEAKER_LIST, description='Speaker:')
",
    "segment_select = widgets.Dropdown(description='Segment:')
",
    "output_plot = widgets.Output()
",
    "output_table = widgets.Output()
",
    "
",
    "def update_segments(*args):
",
    "    speaker_id = speaker_select.value
",
    "    segments = SPEAKER_DATA[speaker_id]
",
    "    segment_select.options = [s['stem'] for s in segments]
",
    "    
",
    "    # Show summary table for speaker
",
    "    with output_table:
",
    "        output_table.clear_output()
",
    "        df_summary = pd.DataFrame(segments)
",
    "        display(df_summary)
",
    "
",
    "def on_selection_change(change):
",
    "    with output_plot:
",
    "        output_plot.clear_output(wait=True)
",
    "        if segment_select.value:
",
    "            plot_alignment(segment_select.value)
",
    "
",
    "speaker_select.observe(update_segments, 'value')
",
    "segment_select.observe(on_selection_change, 'value')
",
    "
",
    "# Initialize
",
    "update_segments()
",
    "
",
    "display(widgets.VBox([speaker_select, segment_select, output_table, output_plot]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
